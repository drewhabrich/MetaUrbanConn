# LEAD NOTE ---------------------------------------------------------------
# The .ris files were 'repaired' e.g. had empty info filled automatically using doi management and 'bibfix' shiny app

# 1. Load relevant packages into library -------------------------------------
library(tidyverse)
library(synthesisr)

# 2. Import search results and deduplicate --------------------------------
bibfiles<-list.files("./raw_data/bibfix_results/", full.names=T) #create a list of all the files in the data directory
bibfiles #should be 6 files
import<-read_refs(filename=bibfiles, return_df=T) #import as a dataframe, so we can manipulate columns

## Let's take a look at the data structure, and clean up a bit before deduplicating
nrow(import) #5187 entries across the 5 databases searched, this is MORE than before, likely due to MORE duplicates
names(import) #some of these columns are useless (and incomplete), some are autogenerated by 'synthesisr'
str(import) #check the structure: dataframe with 21 columns of 'character' type
n_distinct(import$title) #how many distinct articles are there? NOTE: this matches EXACT, 3880
sum(is.na(import$title)) #are any rows missing their titles? 351 have no titles...

# Let's look at any 'missing data' 
## check for NAs by column
import %>% 
  summarise(
    across(everything(), ~ sum(is.na(.)))
  ) #many have few entries; Author, year, and title are missing from ~400 entries, likely news articles.

## Deduplicate based on 'exact' title match (this is the same as n_distinct)
data <- deduplicate(
  import,
  match_by = "title",
  method = "exact" #EXACT MATCHES ONLY, no need to review these.
)
nrow(import)-nrow(data) #How many results were removed this way: 957

## Deduplicate based on text string distance
duplicates_string <- find_duplicates(
  data$title,
  method = "string_osa",
  to_lower = TRUE, #forces all text to lowercase for comparison
  rm_punctuation = TRUE, #removes all punctuation before comparison (useful for papers with subtitles)
  threshold = 5 #the cutoff threshold to decide if the strings are duplicates
)
# we can extract the line numbers from the dataset that are likely duplicated
# this lets us manually review those titles to confirm they are duplicates
manual_check <- review_duplicates(data$title, duplicates_string)
print(manual_check)
View(as_tibble(manual_check)) #manually review if the duplicates are real

# final cleaning
final_dup <- synthesisr::override_duplicates(duplicates_string, 14) #match 14 is actually 2 separate papers, with similar titles
final_res <- extract_unique_references(data, final_dup) #find the unique entries from the culled dataframe, using the list of duplicates

final_res <- final_res %>% filter(!is.na(title)) #remove any rows that had NA in titles, this is an artifact of deduplication.

# 3. Writing the deduplicated bibliography to file ------------------------
#save the final results list into .ris format
final_res

write_refs(final_res,
           format = "ris", #or "bib"
           file = "./output/deduplicated_bib_02-1"
)

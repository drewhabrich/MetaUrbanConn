# 1. Load relevant packages into library -------------------------------------
library(tidyverse)
library(synthesisr)

# 2. Import search results and deduplicate --------------------------------
bibfiles<-list.files("./raw_data/full_search/", full.names=T) #create a list of all the files in the data directory
import<-read_refs(filename=bibfiles, return_df=T) #import as a dataframe, so we can manipulate columns

# how many results were from each each database?
scop <- read_ref("./raw_data/full_search/scopus_search.ris")
wos <- read_refs(filename= c("./raw_data/full_search/wos_s1.ris", "./raw_data/full_search/wos_s2.ris"))
proq <- read_ref("./raw_data/full_search/Proquest_grey.RIS")
biox <- read_ref("./raw_data/full_search/xs-bioRxiv.ris")
ccorg <- read_ref("./raw_data/full_search/ConservationCorridor_mixed.ris")

#total # of entries from initial search
nrow(scop)+nrow(wos)+nrow(proq)+nrow(biox)+nrow(ccorg)
nrow(import) #sum should be the same as the total rows from the import


## Let's take a look at the data structure, and clean up a bit before deduplicating
nrow(import) #4836 entries across the 5 databases searched
names(import) #some of these columns are useless (and incomplete), some are autogenerated by 'synthesisr'
str(import) #check the structure: dataframe with 42 columns of 'character' type
n_distinct(import$title) #how many distinct articles are there? NOTE: this matches EXACT
sum(is.na(import$title)) #are any rows missing their titles? No.

# Let's look at any 'missing data' 
## check for NAs by column
import %>% 
  summarise(
    across(everything(), ~ sum(!is.na(.)))
) #many have few entries

## Deduplicate based on 'exact' title match (this is the same as n_distinct)
data <- deduplicate(
  import,
  match_by = "title",
  method = "exact" #EXACT MATCHES ONLY, no need to review these.
)
nrow(import)-nrow(data) #How many results were removed this way: 948

## Deduplicate based on text string distance
duplicates_string <- find_duplicates(
  data$title,
  method = "string_osa",
  to_lower = TRUE, #forces all text to lowercase for comparison
  rm_punctuation = TRUE, #removes all punctuations before comparison (useful for papers with subtitles)
  threshold = 5 #the cutoff threshold to decide if the strings are duplicates
)
# we can extract the line numbers from the dataset that are likely duplicated
# this lets us manually review those titles to confirm they are duplicates
manual_check <- review_duplicates(data$title, duplicates_string)
print(manual_check)
View(as_tibble(manual_check)) #manually review if the duplicates are real

final_dup <- synthesisr::override_duplicates(duplicates_string, 14) #match 14 is actually 2 separate papers, with similar titles
final_res <- extract_unique_references(data, final_dup) #find the unique entries from the culled dataframe, using the list of duplicates


# 3. Writing the deduplicated bibliography to file ------------------------
data[1,] #this is the first entry
data[1,!is.na(data[1,])] #maybe we remove the NAs?
#save the final results list into .ris format
final_res

write_refs(final_res,
           format = "ris", #or "bib"
           file = "./output/deduplicated_initialsearch_02"
)

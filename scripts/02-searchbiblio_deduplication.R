# LEAD NOTE ---------------------------------------------------------------
# The .ris files were 'repaired' e.g. had empty info filled automatically using doi management and 'bibfix' shiny app

# 1. Load relevant packages into library -------------------------------------
library(tidyverse) #v2.0.0
library(synthesisr) #v0.3.0

# 2. Import search results and deduplicate --------------------------------
bibfiles<-list.files("./raw_data/bibfix_results/", full.names=T) #create a list of all the files in the data directory
bibfiles #should be 6 files
import<-read_refs(filename=bibfiles, return_df=T) %>% filter(!is.na(title)) #import as a dataframe, so we can manipulate columns

scop <- read_ref("./raw_data/bibfix_results/REPAIRED-scopus.ris")
wos <- read_refs(filename= c("./raw_data/bibfix_results/REPAIRED-wos1.ris", "./raw_data/bibfix_results/REPAIRED-wos2.ris"))
proq <- read_ref("./raw_data/bibfix_results/Zotero_Proquest_grey.RIS") %>% filter(!is.na(title)) #this one is wonky, has a bunch of empty rows
biox <- read_ref("./raw_data/bibfix_results/Zotero_bioRxiv.ris")
ccorg <- read_ref("./raw_data/bibfix_results/REPAIRED-conservationcorridor.ris")

nrow(scop)+nrow(wos)+nrow(proq)+nrow(biox)+nrow(ccorg)
nrow(import)

## Let's take a look at the data structure, and clean up a bit before deduplicating
nrow(import) #4836 entries
names(import) #some of these columns are useless (and incomplete), some are autogenerated by 'synthesisr'
glimpse(import) #check the structure: dataframe with 21 columns of 'character' type
n_distinct(import$title) #how many distinct articles are there? NOTE: this matches EXACT
sum(is.na(import$title)) #are any rows missing their titles? 

# Let's look at any 'missing data' 
## check for NAs by column
import %>% summarise(across(everything(), ~ sum(is.na(.)))) %>% tibble 
# many have few entries

## Deduplicate based on 'exact' title match (this is the same as n_distinct)
data <- deduplicate(
  import,
  match_by = "title",
  method = "exact" #EXACT MATCHES ONLY, no need to review these.
)
nrow(import)-nrow(data) #How many results were removed this way: 957

## Deduplicate based on text string distance
duplicates_string <- find_duplicates(
  data$title,
  method = "string_osa",
  to_lower = TRUE, #forces all text to lowercase for comparison
  rm_punctuation = TRUE, #removes all punctuation before comparison (useful for papers with subtitles)
  threshold = 5 #the cutoff threshold to decide if the strings are duplicates
)
# we can extract the line numbers from the dataset that are likely duplicated
# this lets us manually review those titles to confirm they are duplicates
manual_check <- review_duplicates(data$title, duplicates_string)
print(manual_check)
View(as_tibble(manual_check)) #manually review if the duplicates are real

# final cleaning
final_dup <- synthesisr::override_duplicates(duplicates_string, 14) #match 14 is actually 2 separate papers, with similar titles
final_res <- extract_unique_references(data, final_dup) #find the unique entries from the culled dataframe, using the list of duplicates

final_res <- final_res %>% filter(!is.na(title)) #remove any rows that had NA in titles, this is an artifact of deduplication.

# 3. Writing the deduplicated bibliography to file ------------------------
#save the final results list into .ris format
final_res

write_refs(final_res,
           format = "ris", #or "bib"
           file = "./data/deduplicated_bib-02"
)

---
title: "Search results deduplication"
author: "Andrew Habrich"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    fig_caption: yes
    code_folding: show
    code_download: yes
---

Document made with R version `r getRversion()`

```{r load packages, include=FALSE}
# 1. Load relevant packages into library -------------------------------------
library(tidyverse)
library(synthesisr)
```

### 1. Import and summarise search results from the databases

```{r import + sum table, echo=FALSE}
# 2. Import search results and deduplicate --------------------------------
bibfiles<-list.files("./raw_data/full_search/", full.names=T) #create a list of all the files in the data directory
import<-read_refs(filename=bibfiles, return_df=T) #import as a dataframe, so we can manipulate columns

# how many results were from each each database?
scop <- read_ref("./raw_data/full_search/scopus_search.ris")
wos <- read_refs(filename= c("./raw_data/full_search/wos_s1.ris", "./raw_data/full_search/wos_s2.ris"))
proq <- read_ref("./raw_data/full_search/Proquest_grey.RIS")
biox <- read_ref("./raw_data/full_search/xs-bioRxiv.ris")
ccorg <- read_ref("./raw_data/full_search/ConservationCorridor_mixed.ris")

sum_tab <- data.frame(source=c("SCOPUS","Web of Science","ProQuest","bioRxiv","Conservation Corridor","TOTAL"),
                      results=c(nrow(scop),nrow(wos),nrow(proq),nrow(biox),nrow(ccorg), nrow(import)))
as_tibble(sum_tab)
```

SCOPUS and Web of Science are primary literature databases, so it makes sense they would have the most results. ProQuest and bioRxiv are both 'grey literature' sources; Proquest is primarily theses and dissertations, while bioRxiv is a mix of theses and pre-prints. [Conservation corridor](https://conservationcorridor.org/) is an organization that compiles publications and information about landscape connectivity and corridor design.

### 2. Take a look at the data structure, and clean up a bit before deduplicating
#### 2.1. Let's look at any 'missing data'

```{r NA check, echo=TRUE}
## check for NAs by column
import %>% 
  summarise(across(everything(), ~ sum(is.na(.)))) %>% 
  as_tibble()
```

Many columns have few entries, and there are a lot of autogenerated columns that aren't useful.\


#### 2.2. Deduplicate based on 'exact' title match (this is the same as n_distinct() in tidyverse)
```{r exact-dedup}
data <- deduplicate(
  import,
  match_by = "title",
  method = "exact" #EXACT MATCHES ONLY, no need to review these.
)
```

How many results were removed using exact title matching: *`r nrow(import)-nrow(data)`*

#### 2.3. Deduplicate based on text string distance (if the characters match, case-insensitive)

```{r string dedup}
duplicates_string <- find_duplicates(
  data$title,
  method = "string_osa",
  to_lower = TRUE, #forces all text to lowercase for comparison
  rm_punctuation = TRUE, #removes all punctuations before comparison (useful for papers with subtitles)
  threshold = 5 #the cutoff threshold to decide if the strings are duplicates
)
# we can extract the line numbers from the dataset that are likely duplicated
# this lets us manually review those titles to confirm they are duplicates
manual_check <- review_duplicates(data$title, duplicates_string)
as_tibble(manual_check) 

final_dup <- synthesisr::override_duplicates(duplicates_string, 14) #match 14 is actually 2 separate papers, with similar titles
final_res <- extract_unique_references(data, final_dup) #find the unique entries from the culled dataframe, using the list of duplicates
```

After manually checking the list of duplicates, only 1 entry wasn't a 'real duplicate', i.e. two different articles with similar titles (usually suggests one is a response article to the other).

How many duplicated results are removed? Exact matches **`r nrow(import)-nrow(data)`** - String matches **`r nrow(data)-nrow(final_res)`** = **`r nrow(import)-nrow(data) + nrow(data)-nrow(final_res)`**

Now we can write the deduplicated bibliography to file for initial and full screening.

### 3. Writing the deduplicated bibliography to file 

```{r write biblio, eval=FALSE, echo=TRUE}
#save the final results list into .ris format
final_res

write_refs(final_res,
           format = "ris", #or "bib"
           file = "./output/deduplicated_initialsearch_02"
)
```

The generated deduplicated bibliography can be used for the initial round of screening (based on titles and abstracts).

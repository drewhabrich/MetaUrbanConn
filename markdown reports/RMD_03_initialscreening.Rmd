---
title: "Search results bibliography screening: Title and abstract"
author: "Andrew Habrich"
date: '`r Sys.Date()`'
output: 
  html_document: 
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    fig_caption: yes
    code_folding: hide
    code_download: yes
---

```{css zoom-clickfig-src, echo = FALSE}
script src = "https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"
```

```{js zoom-clickfig-jquery, echo = FALSE}
 $(document).ready(function() {
    $('body').prepend('<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>');
    // onClick function for all plots (img's)
    $('img:not(.zoomImg)').click(function() {
      $('.zoomImg').attr('src', $(this).attr('src')).css({width: '100%'});
      $('.zoomDiv').css({opacity: '1', width: 'auto', border: '1px solid white', borderRadius: '5px', position: 'fixed', top: '50%', left: '50%', marginRight: '-50%', transform: 'translate(-50%, -50%)', boxShadow: '0px 0px 50px #888888', zIndex: '50', overflow: 'auto', maxHeight: '100%'});
    });
    // onClick function for zoomImg
    $('img.zoomImg').click(function() {
      $('.zoomDiv').css({opacity: '0', width: '0%'}); 
    });
  });
```

```{r Scrollable text output, include=FALSE}
options(width = 60)
local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})
```

```{r colour text option, include=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

Click figures to zoom in. The document works best on Chrome or Firefox. You can also download the .RMD file by clicking on the button at the top-right corner of the web page.\
Document made with R version `r getRversion()`

### 1. Required R packages

```{r Load relevant packages into library, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
## install.packages("tidyverse","metagear","rcrossref) install packages as needed
library(plyr) #plyr needs to be loaded BEFORE tidyverse to prevent issues with dplyr
library(tidyverse) #load tidyverse
library(synthesisr) #import bibliographic data - part of metaverse
library(lubridate) #this should be in the tidyverse, if for some reason it isn't you can install it separately
library(metagear) #abstract/title screening package
library(rcrossref) #interface with 'crossref' api
```

### 2. Read in the deduplicated bibliographic data set
```{r read data}
initial_dat <- read_refs(filename = "~/output/deduplicated_bib_02-1.ris", return_df = T)
```

Let's check the structure and clean the data so that we can systematically screen

There are **`r ncol(initial_dat)`** columns. As you can see from the table below, they are all coded as 'character', even the year.

```{r column classes, echo=FALSE, max.height='200px'}
initial_dat %>% imap_dfr(~ tibble(colname = .y, classes = class(.x) %>% str_c(collapse = ", ")))
```

#### 2.1. Data cleaning and exploration

How many NAs are there in the dataframe columns?

```{r dataframe NAs by col, echo=FALSE, max.height='200px'}
initial_dat %>% 
  summarise(across(everything(), ~ sum(is.na(.))) %>% 
      as_tibble()) %>% 
      pivot_longer(everything(), names_to="Column", values_to = "# of NAs")
```

```{r preliminary cleaning, include=FALSE}
# remove the unnecessary columns...
initial_dat <- initial_dat %>% select(-c(keywords,notes))

# Check the entries that are missing their titles, author, year, doi, abstract
initial_dat %>% filter(is.na(title)) %>% as_tibble() #NONE!
initial_dat %>% filter(is.na(author)) %>% as_tibble()
initial_dat %>% filter(is.na(year)) %>% as_tibble()

initial_dat <- initial_dat %>% mutate(year=as.numeric(year)) %>% as_tibble() #change year to be numeric instead of character
dat<-initial_dat 
```

A number of other data cleaning steps are done too; The descriptive steps taken are in script 03-screening_metagear available on [my GitHub page by clicking here](https://github.com/drewhabrich/MetaUrbanConn).

```{r filling in missing data using crossref, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
### Is there a pattern to which entries are missing DOI? Or URLS?
## 2.1.1 DOI MISSING
dat %>% filter(is.na(doi)) %>% ggplot(aes(x = year)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "white") +
  labs(x = "Year", y = "Publication count", title = "Search results with missing DOI by year") + xlim(1980,NA)
# where are these entries missing other data?
dat %>% filter(is.na(doi)) %>% summarise(across(everything(), ~ sum(is.na(.)))) #all missing doi are missing the journal entry too...
dat %>% filter(is.na(doi)) %>% group_by(source) %>% count(source) 
#Results from PROQUEST put the source here instead of 'Journal' likely because there are a number of theses/dissertations that weren't published in a peer-reviewed journal
# visualize frequencies with a plot 
dat %>% filter(is.na(doi)) %>% group_by(source) %>% count(source) %>% 
  ggplot(aes(y = reorder(source, n), x=n)) + #reorder the y-axis by the n column (frequency)
  geom_bar(stat="identity", fill = "steelblue", color = "white") +
  labs(x = "Frequency", y = "Source", title = "Frequency of entries by source") +
  theme(axis.text.y = element_text(size=5, hjust=1))
# lets fill the empty cells in journal column with the data from source column.
dat$journal <- ifelse(is.na(dat$journal), dat$source, dat$journal) #if the cell is NA, replace with value from source column, otherwise use journal

## 2.1.2 Journal MISSING
# What entries are STILL missing journal info?
dat %>% filter(is.na(journal)) %>% count(source_type) #139 that are entered as JOUR!
dat %>% filter(is.na(journal)) %>% filter(source_type=="JOUR") %>% summarise(sum(is.na(url))) #39 entries don't have a URL, the others are from SCOPUS
dat %>% filter(is.na(journal)) %>% filter(source_type=="JOUR") %>% summarise(sum(is.na(doi))) #all are missing their DOIs

## 2.1.3 URL MISSING
dat %>% filter(is.na(url)) %>% ggplot(aes(x = year)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "white") +
  labs(x = "Year", y = "Publication count", title = "Search results with missing URLS by year") + xlim(1980,NA) #HUGE gap in 2022, why is that...

dat %>% filter(is.na(url)) %>% filter(year=="2022") #They have DOIs so its fine if missing URL
# where are these entries missing other data?
dat %>% filter(is.na(url)) %>% summarise(across(everything(), ~ sum(is.na(.)))) #all missing doi are missing the journal entry too...

### 2.2 FILL MISSING BIBLIOGRAPHIC DATA ---------------------------------------
## Can we populate the missing cells based on doi, title, author/year using 'crossref' package?
dat %>% filter(is.na(doi)) %>% nrow() #682 rows missing DOI
  
# fill missing years based on doi
year_doi<- dat %>% filter(is.na(year)) %>% select(doi,title) #filter only the entries with missing year
year_d<-cr_works(dois=year_doi$doi)$data #extract data from crossref based on  year

year_doi$year <- year(ymd(year_d$published.online)) #classify as date and extract the year

dat <- dat %>%
  left_join(year_doi %>% select(doi, year), by = "doi") %>% #
  mutate(year = if_else(is.na(year.x), year.y, year.x)) %>% #
  select(-year.x, -year.y) %>% #remove the redundant columns
  relocate(year, .after = author) #place year column after author column

dat %>% 
  summarise(
    across(everything(), ~ sum(is.na(.))) %>% as_tibble()
  )

```

### 3. Bibliography data exploration

Let's take a peek at the first couple of rows:

```{r data structure & exploration}
#Check the data structure
head(dat)
```

#### Let's think about some data exploration questions to get familiar with the data

##### 3.1 How many publications per year are there on this topic?

```{r pub x year fig, echo=FALSE, message=FALSE, warning=FALSE}
dat %>% ggplot(aes(x = year)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "white") +
  labs(x = "Year", y = "Publication count", title = "'Urban landscape connectivity' search results by year") + 
  scale_x_continuous(limits=c(1980,2024), breaks=seq(1980,2024,4))+
  theme_bw()
```

##### 3.2 How many authors/articles are there?

```{r author/article, echo=FALSE, message=FALSE, warning=FALSE}
#n_distinct(dat$author) #this is only a DIRECT match.
# Count the number of authors in each row, handling both ";" and "and" as separators; NOTE: this uses regular expression to find text strings
dat$num_authors <- sapply(strsplit(dat$author, ";\\s*|\\s+and\\s+"), length)
# what is the range of # of authors
dat %>% ggplot(aes(x = num_authors)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "white") +
  labs(x = "# of authors on publication", y = "Frequency") + scale_x_continuous(limits=c(1,30), breaks=seq(1,30,2))+
  theme_bw()
```

\
**Zero single authored papers!**

```{r eval=FALSE, include=FALSE}
dat %>% summarise(mean_numauth=mean(num_authors),
                  med_numauth=median(num_authors),
                  min_numauth=min(num_authors),
                  max_numauth=max(num_authors)) #the entry with the max # of authors is a dataset publication with 199 people!
```

##### 3.3 How many different journals are the entries published in? (NOTE: Some entries do not have a journal; i.e. preprint, thesis, or non-peer reviewed articles)

```{r entries by journal, echo=FALSE, message=FALSE, warning=FALSE}
#n_distinct(dat$journal) #distinct journal entries (SOME MAY BE DUPLICATEDDD, this only matches on EXACT)
dat %>% 
  group_by(journal) %>% 
  count(journal) %>% 
  filter(n >= 5) %>% 
  filter(!is.na(journal)) %>% 
  ggplot(aes(x=n, y = reorder(journal,n))) + #reorder to descending frequency
    geom_bar(stat="identity", fill = "steelblue", color = "white") +
    labs(x = "Frequency", y = "Journal", title = "Frequency of entries by Journal (NAs omitted)") +
    scale_y_discrete(labels = function(x) str_trunc(x, 50)) +
    theme(axis.text.y = element_text(size=4)) +
    theme_bw()
```

Note how some journals appear multiple times (in all upper-case or title-case). Have to fix this for later plots.

### 4. Title and abstract screening using the MetaGear package v.`r packageVersion("metagear")`

```{r screening GUI, eval=FALSE, include=FALSE}
### Create the review excel sheet and initialize format for abstract screening
# bib_unscreened <- effort_distribute(dat, reviewers="Drew", initialize = T, save_split = T, directory = "./output/") #only need to rune once, uncomment if needed
# colnames(bib_unscreened)

### 4.1 Use the built in GUI for metaGear to screen articles -----
# NOTE: YOU HAVE TO SAVE BEFORE QUITTING THE GUI OR YOU'LL LOSE PROGRESS, it will update the effort_*reviewer*.csv file
abstract_screener(file="./output/effort_Drew.csv",
                  aReviewer = "Drew",
                  unscreenedColumnName = "INCLUDE",
                  unscreenedValue = "not vetted",
                  abstractColumnName = "abstract",
                  titleColumnName = "title",
                  browserSearch = "https://scholar.google.ca/scholar?hl=en&as_sdt=0%252C5&q=",
                  fontSize = 14,
                  windowWidth = 85,
                  windowHeight = 20,
                  highlightKeywords = c("urban","city","connectivity","permeability","network","isolat",
                                        "species richness", "diversity","corridor","species", "least cost"),
                  highlightColor = "palegoldenrod",
                  theButtons = c("YES","MAYBE","NOTurban","NOTconn","REVIEW"),
                  keyBindingToButtons = c("q","w","e","r","t")) 
```

#### 4.1 Screening effort summary

```{r screened results vis, message=FALSE, warning=FALSE, echo=FALSE}
bibscreened<-effort_merge(directory="./output")
#unique(bibscreened$INCLUDE) #how many categories are there?

# generate a summary table by each decision category
screen_sum<-bibscreened %>% 
  group_by(INCLUDE) %>% #group by category
  summarize(count=n()) %>% #summarize based on count of each category
  mutate(percentage = signif(count/nrow(bibscreened)*100, digits=2), #estimate the percentage in each category
         summary = case_when(INCLUDE == "NOTconn" ~ "Excluded based title, no indication of 'urban' or 'landscape connectivity'",
                             INCLUDE == "NOTurban" ~ "Excluded based on abstract, not urban or no measure of connectivity",
                             INCLUDE == "MAYBE" ~ "Unclear eligibility, needs full text screening",
                             INCLUDE == "YES" ~ "Potential candidate studies identified",
                             INCLUDE == "REVIEW" ~ "Review/Methodology/Framework articles",
                             TRUE ~ "IN PROGRESS, not vetted yet")) %>% 
  mutate(INCLUDE=if_else(INCLUDE=="NOTconn","TitleExcl", if_else(INCLUDE=="NOTurban","AbstrExcl", INCLUDE))) %>% 
  arrange(desc(percentage)) #arrange in descending % order
  
kableExtra::kable(screen_sum, caption = "Summary of screening, grouped by decision category",
      col.names=c("Decision","n","% total","Description")) %>%  
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = F)
```

```{r Jname for YES, include=FALSE}
# visualize what journals these groups are in published in
bibscreened %>% 
  filter(INCLUDE=="YES") %>% 
  group_by(journal) %>% 
  count(journal) %>% 
  ggplot(aes(x=n, y = reorder(journal,n))) + #reorder to descending frequency
  geom_bar(stat="identity", fill = "steelblue", color = "white") +
  labs(x = "Frequency", y = "Journal", title = "Frequency of entries by Journal for potential candidate studies") +
  scale_y_discrete(labels = function(x) str_trunc(x, 35)) +
  theme(axis.text.y = element_text(size=4))  
# There appears to be some case-sensitivity issues in the journal names, likely because they came from a variety of sources

# visualize on journal that ISN'T CASE-SENSITIVE
# bibscreened %>%
#   filter(INCLUDE == "YES") %>% 
#   mutate(journal_lower = str_to_lower(journal)) %>%
#   group_by(journal_lower) %>%
#   summarise(count = n()) %>%
#   ungroup() %>%
#   arrange(desc(count)) %>%
#   ggplot(aes(x = count, y = reorder(journal_lower, count))) +
#   geom_bar(stat = "identity", fill = "steelblue", color = "white") +
#   labs(x = "Frequency", y = "Journal", title = "Frequency of entries by Journal (case-insensitive)") +
#   theme(axis.text.y = element_text(size = 10))
```

```{r Clean journal title, include=FALSE}
bibs <- bibscreened %>% 
  mutate(journal = str_replace_all(journal, "&", "and")) %>% 
  mutate(jour_s = str_to_lower(journal)) %>%
  mutate(jour_s = str_to_title(jour_s)) %>% #coerce to title-case
  mutate(jour_s = str_replace_all(jour_s, "\\b(And|In|Of|The|For)\\b", str_to_lower)) #capitalized conjuctions to lowercase
## How many journals were consolidated during string cleaning? (before - after)
n_distinct(unique(bibscreened$journal)) - n_distinct(unique(bibs$jour_s)) 
```

#### 4.2 For articles coded as 'YES' (for full text screening), what journals are they in?

```{r yes-articles, echo=FALSE}
bibs %>% 
  filter(INCLUDE=="YES") %>% 
  group_by(jour_s) %>% 
  count(jour_s) %>% 
  ggplot(aes(x=n, y = reorder(jour_s,n))) + #reorder to descending frequency
  geom_bar(stat="identity", fill = "steelblue", color = "black") +
  labs(x = "Frequency", y = "Journal", title = "Frequency of entries by Journal") +
  theme_bw()+ #to standardize to a common theme
  theme(axis.text.y = element_text(size = 4)) + #specific modifications to the theme
  scale_y_discrete(labels = function(x) str_trunc(x, 35)) + #truncate the entries with long names
  coord_cartesian(xlim=c(0, 55), expand=T)
```

```{r run-dontshow, message=FALSE, warning=FALSE, include=FALSE}
## How many entries are there for the categories
### For entries in the YES category, #What is the maximum number of articles from 1 journal?
bibs %>% filter(INCLUDE=="YES") %>% 
    group_by(jour_s) %>% count(jour_s) %>% 
    arrange(desc(n)) 
### How many journals have more than 5 entries (across all decision groups)
bibs %>% filter(INCLUDE!="not vetted") %>% 
    group_by(jour_s) %>% count(jour_s) %>% 
    #arrange(desc(n)) %>% #this will arrange the table in descending order if you don't run the line below; uncomment if needed
    filter(n >= 5) %>% n_distinct() 

## Plot all the INCLUDE categories
### pull the list of journals with >5 entries
jlist<-bibs %>% filter(INCLUDE!="not vetted") %>% 
  group_by(jour_s) %>% count(jour_s) %>% 
  #arrange(desc(n)) %>% #this will arrange the table in descending order if you don't run the line below; uncomment if needed
  filter(n >= 5) %>% pull(jour_s)
```

#### 4.3 For journals with \>5 entries, what is the breakdown by each decision group?

```{r decision by jour_s, echo=FALSE, message=FALSE, warning=FALSE}
bibs %>% 
  filter(INCLUDE!="not vetted") %>%
  filter(jour_s %in% jlist) %>% #un/comment this line to get ALL the journals, very cluttered with all. 
  group_by(jour_s, INCLUDE) %>%
  summarise(count = n()) %>%
  ungroup() %>% 
  mutate(INCLUDE=if_else(INCLUDE=="NOTconn","TitleExcl", if_else(INCLUDE=="NOTurban","AbstrExcl", INCLUDE))) %>% 
  ggplot(aes(x= count, y = reorder(jour_s,count), fill = INCLUDE)) + #reorder to descending frequency
    geom_bar(stat="identity", position = "stack", colour="black") +
    labs(x = "Frequency", y = "Journal", title = "Frequency of entries by Journal (with n >= 5)") +
    theme_bw()+ #to standardize to a common theme
    theme(axis.text.y = element_text(size = 4)) + #specific modifications to the theme
    scale_y_discrete(labels = function(x) str_trunc(x, 35)) +
    coord_cartesian(xlim=c(0, 120), expand=T)
```

### 5. Next steps

Several issues still need to be resolved before moving on to data extraction:\
- Full-text reading for all the 'MAYBE's\
- Full-text download for all the potential candidate studies\
- Chase citations from key reviews in the 'REVIEW' group

For data extraction:\
- Put together team of people to help flag and extract data\
- Create extraction sheet and instructions/meta-data for extractions
